{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing important libraries and modules\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the questions\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def clean_text(row):\n",
    "\n",
    "#    row = re.sub(\"[^a-zA-Z]\", \" \", row) \n",
    "#    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#    tokenizer.tokenize(row)\n",
    "\n",
    "    word_list = row.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    \n",
    "#    print(word_list)\n",
    "    ps=PorterStemmer();\n",
    "    # l = WordNetLemmatizer()\n",
    "    \n",
    "    # meaningful_words = [l.lemmatize(w) for w in word_list if not w in stops]       \n",
    "    meaningful_words = [ps.stem(w) for w in word_list if not w in stops]   \n",
    "    \n",
    "    # Joining the tokenized words back into one string\n",
    "    m = set(meaningful_words)\n",
    "    meaningful_words = list(m)\n",
    "    \n",
    "    return( \" \".join( meaningful_words ))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill the null values with some random string\n",
    "\n",
    "train_data['question1'].fillna('xpx',inplace=True)\n",
    "train_data['question2'].fillna('xpx',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the train data\n",
    "\n",
    "train_data['question1_cleaned'] = train_data.apply(lambda row : clean_text(row['question1']),axis=1)\n",
    "train_data['question2_cleaned'] = train_data.apply(lambda row : clean_text(row['question2']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replacing the null questions with some random string\n",
    "\n",
    "test_data['question1'].fillna('xpx',inplace=True)\n",
    "test_data['question2'].fillna('xpx',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning the test data\n",
    "\n",
    "test_data['question1_cleaned'] = test_data.apply(lambda row : clean_text(row['question1']),axis=1)\n",
    "test_data['question2_cleaned'] = test_data.apply(lambda row : clean_text(row['question2']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the cleaned data\n",
    "\n",
    "train_data.to_csv('cleaned_train.csv',index = False)\n",
    "test_data.to_csv('cleaned_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing the cosing similarity metric\n",
    "\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])  # sigma (a.b)\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2) # sigma(sqrt(a^2))* sigma(sqrt(b^2))\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "def get_similarity(row):\n",
    "    vector1 = text_to_vector(train_data.loc[row]['question1_cleaned'])\n",
    "    vector2 = text_to_vector(train_data.loc[row]['question2_cleaned'])\n",
    "    \n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    return cosine\n",
    "\n",
    "def get_similarity_test(row):\n",
    "    vector1 = text_to_vector(test_data.loc[row]['question1_cleaned'])\n",
    "    vector2 = text_to_vector(test_data.loc[row]['question2_cleaned'])\n",
    "    \n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating the cosine similarity \n",
    "\n",
    "train_data['cosine_similarity'] = train_data.apply(lambda row : get_similarity(row['id']), axis = 1)\n",
    "test_data['cosine_similarity'] = test_data.apply(lambda row : get_similarity_test(row['test_id']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature - Shared Words\n",
    "\n",
    "def shared_words(row):\n",
    "    q1 = row['question1_cleaned'].split()\n",
    "    q2 = row['question2_cleaned'].split()\n",
    "    shared_words_in_q1 = [w for w in q1 if w in q2]\n",
    "    shared_words_in_q2 = [w for w in q2 if w in q1]\n",
    "    R = float((len(shared_words_in_q1) + len(shared_words_in_q2)))/float((len(q1) + len(q2)))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computing the Shared Words\n",
    "train_data['shared_words'] = train_data.apply(shared_words, axis = 1, raw = True)\n",
    "test_data['shared_words'] = test_data.apply(shared_words, axis = 1, raw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_qs = pd.Series(train_data['question1'].tolist() + train_data['question2'].tolist()).astype(str)\n",
    "test_qs = pd.Series(test_data['question1'].tolist() + test_data['question2'].tolist()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=5000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return float(1) / float((count + eps))\n",
    "\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1_cleaned']).lower().split():\n",
    "        q1words[word] = 1\n",
    "    for word in str(row['question2_cleaned']).lower().split():\n",
    "        q2words[word] = 1\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['tfidf_count'] = train_data.apply(lambda x: tfidf(x),axis =1 )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['tfidf_count'] = test_data.apply(lambda x: tfidf(x),axis =1 )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['tfidf_count'].fillna('0',inplace = True )\n",
    "test_data['tfidf_count'].fillna('0',inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing some basic features - TRAIN\n",
    "\n",
    "train_data['length_question1'] = train_data.apply(lambda x: len(x['question1_cleaned']),axis=1)\n",
    "train_data['length_question2'] = train_data.apply(lambda x: len(x['question2_cleaned']),axis=1)\n",
    "train_data['word_count_question1'] = train_data.apply(lambda x: len(x['question1_cleaned'].split()),axis=1)\n",
    "train_data['word_count_question2'] = train_data.apply(lambda x: len(x['question2_cleaned'].split()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing some basic features - TEST\n",
    "\n",
    "test_data['length_question1'] = test_data.apply(lambda x: len(x['question1_cleaned']),axis=1)\n",
    "test_data['length_question2'] = test_data.apply(lambda x: len(x['question2_cleaned']),axis=1)\n",
    "test_data['word_count_question1'] = test_data.apply(lambda x: len(x['question1_cleaned'].split()),axis=1)\n",
    "test_data['word_count_question2'] = test_data.apply(lambda x: len(x['question2_cleaned'].split()),axis=1)\n",
    "train_data['length_difference'] = abs(train_data['length_question1'] - train_data['length_question2'])\n",
    "train_data['word_count_difference'] = abs(train_data['word_count_question1'] - train_data['word_count_question2'])\n",
    "test_data['length_difference'] = abs(test_data['length_question1'] - test_data['length_question2'])\n",
    "test_data['word_count_difference'] = abs(test_data['word_count_question1']-test_data['word_count_question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computing some fuzzy features - Train\n",
    "\n",
    "# fuzz_ratio : number of matches / total number of characters in both string\n",
    "# fuzz_partial_ratio : finds the smallest of the two string, searches it in the other\n",
    "# fuzz_token_sort_ratio : sorts the strings first, and then finds the ratio as fuzz_ratio\n",
    "# fuzz_token_set_ratio : operates on the same principles as a set\n",
    "\n",
    "train_data['fuzz_ratio'] = train_data.apply(lambda x: fuzz.ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "train_data['fuzz_QRatio'] = train_data.apply(lambda x: fuzz.QRatio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "train_data['fuzz_WRatio'] = train_data.apply(lambda x: fuzz.WRatio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "train_data['fuzz_partial_ratio'] = train_data.apply(lambda x: fuzz.partial_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "train_data['fuzz_token_set_ratio'] = train_data.apply(lambda x: fuzz.token_set_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "train_data['fuzz_token_sort_ratio'] = train_data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "train_data['fuzz_partial_token_set_ratio'] = train_data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "train_data['fuzz_partial_token_sort_ratio'] = train_data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing some fuzzy features - Train\n",
    "\n",
    "# fuzz_ratio : number of matches / total number of characters in both string\n",
    "# fuzz_partial_ratio : finds the smallest of the two string, searches it in the other\n",
    "# fuzz_token_sort_ratio : sorts the strings first, and then finds the ratio as fuzz_ratio\n",
    "# fuzz_token_set_ratio : operates on the same principles as a set\n",
    "\n",
    "test_data['fuzz_ratio'] = test_data.apply(lambda x: fuzz.ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "test_data['fuzz_QRatio'] = test_data.apply(lambda x: fuzz.QRatio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "\n",
    "###test_data['fuzz_WRatio'] = test_data.apply(lambda x: fuzz.WRatio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "###test_data['fuzz_partial_ratio'] = test_data.apply(lambda x: fuzz.partial_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "\n",
    "test_data['fuzz_token_set_ratio'] = test_data.apply(lambda x: fuzz.token_set_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "test_data['fuzz_token_sort_ratio'] = test_data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "\n",
    "###test_data['fuzz_partial_token_set_ratio'] = test_data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)\n",
    "####test_data['fuzz_partial_token_sort_ratio'] = test_data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1_cleaned']),str(x['question2_cleaned'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the cleaned data\n",
    "train_data.to_csv('train_features.csv',index = False)\n",
    "test_data.to_csv('test_features.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404288, 6)\n",
      "(404288,)\n",
      "(2345796, 6)\n"
     ]
    }
   ],
   "source": [
    "# Dividing data into train and test - Cosine and Shared\n",
    "\n",
    "columns = ['tfidf_count','cosine_similarity','shared_words','fuzz_token_sort_ratio','fuzz_token_set_ratio','length_difference']\n",
    "X_train = train_data[columns]\n",
    "Y_train = train_data['is_duplicate']\n",
    "X_test = test_data[columns]                     \n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_features.csv')\n",
    "test_data = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data.values\n",
    "x = train_data[0::,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.72460088e+01,   1.89564539e+00,  -3.80471719e+00,\n",
       "         -1.26492716e-01],\n",
       "       [ -7.46456940e-01,   2.79511786e+01,   2.04883464e+00,\n",
       "          3.37795216e-02],\n",
       "       [  5.19126508e+00,  -8.30273658e+00,  -4.76960753e+00,\n",
       "          2.24868661e-01],\n",
       "       ..., \n",
       "       [ -2.18565015e+01,   3.36766753e+00,   1.46634857e+01,\n",
       "         -3.55902187e-02],\n",
       "       [  5.64572104e+01,  -1.22711976e+01,  -1.24564507e+00,\n",
       "          8.96017836e-02],\n",
       "       [ -4.47139893e+01,  -4.64045422e+00,  -8.10850365e+00,\n",
       "         -1.56904042e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA Implementation\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 4)\n",
    "pca.fit(X_train)\n",
    "pca.fit_transform(X_train)\n",
    "pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['tfidf_count','cosine_similarity','shared_words','fuzz_token_sort_ratio','fuzz_token_set_ratio','length_difference','is_duplicate']\n",
    "X_train = train_data[columns]\n",
    "\n",
    "train = X_train.values\n",
    "\n",
    "X = train[0::, 1::]\n",
    "y = train[0::, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNeighborsClassifier': 0.56742982654125174}\n",
      "{'DecisionTreeClassifier': 2.6550827027120696, 'KNeighborsClassifier': 0.56742982654125174}\n",
      "{'RandomForestClassifier': 0.50787270705249188, 'DecisionTreeClassifier': 2.6550827027120696, 'KNeighborsClassifier': 0.56742982654125174}\n",
      "{'RandomForestClassifier': 0.50787270705249188, 'MultinomialNB': 2.5644549149400513, 'DecisionTreeClassifier': 2.6550827027120696, 'KNeighborsClassifier': 0.56742982654125174}\n",
      "{'RandomForestClassifier': 0.50787270705249188, 'MultinomialNB': 2.5644549149400513, 'DecisionTreeClassifier': 2.6550827027120696, 'LinearDiscriminantAnalysis': 0.56168599145938081, 'KNeighborsClassifier': 0.56742982654125174}\n",
      "{'LogisticRegression': 0.55952732307627429, 'RandomForestClassifier': 0.50787270705249188, 'KNeighborsClassifier': 0.56742982654125174, 'LinearDiscriminantAnalysis': 0.56168599145938081, 'DecisionTreeClassifier': 2.6550827027120696, 'MultinomialNB': 2.5644549149400513}\n",
      "{'LogisticRegression': 0.55952732307627429, 'RandomForestClassifier': 0.50787270705249188, 'KNeighborsClassifier': 1.1440882445139917, 'LinearDiscriminantAnalysis': 0.56168599145938081, 'DecisionTreeClassifier': 2.6550827027120696, 'MultinomialNB': 2.5644549149400513}\n",
      "{'LogisticRegression': 0.55952732307627429, 'RandomForestClassifier': 0.50787270705249188, 'KNeighborsClassifier': 1.1440882445139917, 'LinearDiscriminantAnalysis': 0.56168599145938081, 'DecisionTreeClassifier': 5.3801578713281071, 'MultinomialNB': 2.5644549149400513}\n",
      "{'LogisticRegression': 0.55952732307627429, 'RandomForestClassifier': 1.0169538160818177, 'KNeighborsClassifier': 1.1440882445139917, 'LinearDiscriminantAnalysis': 0.56168599145938081, 'DecisionTreeClassifier': 5.3801578713281071, 'MultinomialNB': 2.5644549149400513}\n",
      "{'LogisticRegression': 0.55952732307627429, 'RandomForestClassifier': 1.0169538160818177, 'KNeighborsClassifier': 1.1440882445139917, 'LinearDiscriminantAnalysis': 0.56168599145938081, 'DecisionTreeClassifier': 5.3801578713281071, 'MultinomialNB': 5.1496755080291026}\n",
      "{'LogisticRegression': 0.55952732307627429, 'RandomForestClassifier': 1.0169538160818177, 'KNeighborsClassifier': 1.1440882445139917, 'LinearDiscriminantAnalysis': 1.1237819295286853, 'DecisionTreeClassifier': 5.3801578713281071, 'MultinomialNB': 5.1496755080291026}\n",
      "{'LogisticRegression': 1.1197003587291321, 'RandomForestClassifier': 1.0169538160818177, 'KNeighborsClassifier': 1.1440882445139917, 'LinearDiscriminantAnalysis': 1.1237819295286853, 'DecisionTreeClassifier': 5.3801578713281071, 'MultinomialNB': 5.1496755080291026}\n",
      "{'LogisticRegression': 1.1197003587291321, 'RandomForestClassifier': 1.0169538160818177, 'KNeighborsClassifier': 1.7112497923418428, 'LinearDiscriminantAnalysis': 1.1237819295286853, 'DecisionTreeClassifier': 5.3801578713281071, 'MultinomialNB': 5.1496755080291026}\n",
      "{'LogisticRegression': 1.1197003587291321, 'RandomForestClassifier': 1.0169538160818177, 'KNeighborsClassifier': 1.7112497923418428, 'LinearDiscriminantAnalysis': 1.1237819295286853, 'DecisionTreeClassifier': 8.0478768690368518, 'MultinomialNB': 5.1496755080291026}\n",
      "{'LogisticRegression': 1.1197003587291321, 'RandomForestClassifier': 1.5237573035217107, 'KNeighborsClassifier': 1.7112497923418428, 'LinearDiscriminantAnalysis': 1.1237819295286853, 'DecisionTreeClassifier': 8.0478768690368518, 'MultinomialNB': 5.1496755080291026}\n",
      "{'LogisticRegression': 1.1197003587291321, 'RandomForestClassifier': 1.5237573035217107, 'KNeighborsClassifier': 1.7112497923418428, 'LinearDiscriminantAnalysis': 1.1237819295286853, 'DecisionTreeClassifier': 8.0478768690368518, 'MultinomialNB': 7.6798136539711361}\n",
      "{'LogisticRegression': 1.1197003587291321, 'RandomForestClassifier': 1.5237573035217107, 'KNeighborsClassifier': 1.7112497923418428, 'LinearDiscriminantAnalysis': 1.6811269263300492, 'DecisionTreeClassifier': 8.0478768690368518, 'MultinomialNB': 7.6798136539711361}\n",
      "{'LogisticRegression': 1.6748284139518594, 'RandomForestClassifier': 1.5237573035217107, 'KNeighborsClassifier': 1.7112497923418428, 'LinearDiscriminantAnalysis': 1.6811269263300492, 'DecisionTreeClassifier': 8.0478768690368518, 'MultinomialNB': 7.6798136539711361}\n"
     ]
    }
   ],
   "source": [
    "# Ensembling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors = 30),\n",
    "    DecisionTreeClassifier(max_depth=100, min_samples_leaf=6),\n",
    "    RandomForestClassifier(max_depth = 20, n_estimators = 150),\n",
    "    MultinomialNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    LogisticRegression()]\n",
    "\n",
    "log_cols = [\"Classifier\", \"Accuracy\"]\n",
    "log  = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "cv = cross_validation.KFold(len(X_train), n_folds=4)\n",
    "\n",
    "acc_dict = {}\n",
    "\n",
    "for train_index, test_index in cv:    \n",
    "    for clf in classifiers:\n",
    "        name = clf.__class__.__name__\n",
    "        clf.fit(X_train.loc[train_index], Y_train.loc[train_index])\n",
    "        train_predictions = clf.predict_proba(X_train.loc[test_index])\n",
    "        acc = log_loss(Y_train.loc[test_index], train_predictions,eps=1e-15)\n",
    "        if name in acc_dict:\n",
    "            acc_dict[name] += acc\n",
    "        else:\n",
    "            acc_dict[name] = acc    \n",
    "        print(acc_dict)\n",
    "    \n",
    "for clf in acc_dict:\n",
    "    acc_dict[clf] = acc_dict[clf] / 3.0\n",
    "    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
    "    log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DecisionTreeClassifier': 2.6826256230122838,\n",
       " 'KNeighborsClassifier': 0.5704165974472809,\n",
       " 'LinearDiscriminantAnalysis': 0.56037564211001645,\n",
       " 'LogisticRegression': 0.55827613798395315,\n",
       " 'MultinomialNB': 2.5599378846570455,\n",
       " 'RandomForestClassifier': 0.5079191011739036}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>11.623961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>9.898493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>11.816099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "      <td>11.578505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>10.841690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>10.827243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier   Accuracy\n",
       "0          LogisticRegression  11.623961\n",
       "0      RandomForestClassifier   9.898493\n",
       "0                  GaussianNB  11.816099\n",
       "0  LinearDiscriminantAnalysis  11.578505\n",
       "0      DecisionTreeClassifier  10.841690\n",
       "0        KNeighborsClassifier  10.827243"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_classifier = SVC()\n",
    "candidate_classifier.fit(train[0::, 1::], train[0::, 0])\n",
    "result = candidate_classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import chi2\\nX_new = SelectKBest(chi2, k=3).fit_transform(X_train, Y_train)\\nX_new'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature selection - Log Loss : 0.58\n",
    "'''from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X_new = SelectKBest(chi2, k=3).fit_transform(X_train, Y_train)\n",
    "X_new'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 100, 'min_samples_leaf': 6}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {'max_depth':[50,100,150], 'min_samples_leaf':[2, 4, 6]}\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "clf = GridSearchCV(model, parameters)\n",
    "clf.fit(X_train, Y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134763 134764 134765 ..., 404285 404286 404287]\n",
      "[[  4.82756061e-01   5.17243939e-01]\n",
      " [  4.98886217e-01   5.01113783e-01]\n",
      " [  6.03582095e-01   3.96417905e-01]\n",
      " ..., \n",
      " [  9.99967480e-01   3.25203252e-05]\n",
      " [  9.78987417e-01   2.10125829e-02]\n",
      " [  4.49412856e-01   5.50587144e-01]]\n",
      "0.50740266307\n",
      "[     0      1      2 ..., 404285 404286 404287]\n",
      "[[ 0.44687268  0.55312732]\n",
      " [ 0.77030796  0.22969204]\n",
      " [ 0.9905966   0.0094034 ]\n",
      " ..., \n",
      " [ 0.2927395   0.7072605 ]\n",
      " [ 1.          0.        ]\n",
      " [ 0.63508604  0.36491396]]\n",
      "0.510313637871\n",
      "[     0      1      2 ..., 269523 269524 269525]\n",
      "[[ 0.28957091  0.71042909]\n",
      " [ 0.81535107  0.18464893]\n",
      " [ 0.98445238  0.01554762]\n",
      " ..., \n",
      " [ 0.72660816  0.27339184]\n",
      " [ 1.          0.        ]\n",
      " [ 0.28957091  0.71042909]]\n",
      "0.506395749031\n"
     ]
    }
   ],
   "source": [
    "# Random Forest : Log Loss - 0.50\n",
    "cv = cross_validation.KFold(len(X_train), n_folds=3)\n",
    "model = RandomForestClassifier(n_estimators=150,max_depth=20)\n",
    "\n",
    "for traincv, testcv in cv:\n",
    "    print(traincv)\n",
    "    model.fit(X_train.loc[traincv], Y_train.loc[traincv])\n",
    "    rf_predict = model.predict_proba(X_train.loc[testcv]) \n",
    "    print(rf_predict)\n",
    "    print(log_loss(Y_train.loc[testcv], rf_predict, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134763 134764 134765 ..., 404285 404286 404287]\n"
     ]
    }
   ],
   "source": [
    "# SVM - Log Loss : \n",
    "from sklearn.svm import SVR\n",
    "cv = cross_validation.KFold(len(X_train), n_folds=3)\n",
    "model = SVR()\n",
    "\n",
    "for traincv, testcv in cv:\n",
    "    print(traincv)\n",
    "    model.fit(X_train.loc[traincv], Y_train.loc[traincv])\n",
    "    rf_predict = model.predict_proba(X_train.loc[testcv]) \n",
    "    print(rf_predict)\n",
    "    print(log_loss(Y_train.loc[testcv], rf_predict, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cv = cross_validation.KFold(len(X_train), n_folds=3)\\nmodel = neighbors.KNeighborsClassifier(n_neighbors = 30)\\n\\nfor traincv, testcv in cv:\\n    print(traincv)\\n    model.fit(X_train.loc[traincv], Y_train.loc[traincv])\\n    rf_predict = model.predict_proba(X_train.loc[testcv]) \\n    print(rf_predict)\\n    print(log_loss(Y_train.loc[testcv], rf_predict, eps=1e-15))'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN Algorithm - Log Loss : 0.56\n",
    "'''cv = cross_validation.KFold(len(X_train), n_folds=3)\n",
    "model = neighbors.KNeighborsClassifier(n_neighbors = 30)\n",
    "\n",
    "for traincv, testcv in cv:\n",
    "    print(traincv)\n",
    "    model.fit(X_train.loc[traincv], Y_train.loc[traincv])\n",
    "    rf_predict = model.predict_proba(X_train.loc[testcv]) \n",
    "    print(rf_predict)\n",
    "    print(log_loss(Y_train.loc[testcv], rf_predict, eps=1e-15))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparing the Submission File \n",
    "\n",
    "pred = model.predict_proba(X_test)\n",
    "\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df['test_id'] = test_data['test_id']\n",
    "sub_df['is_duplicate'] = pred[:,1]\n",
    "sub_df.to_csv('quora_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
